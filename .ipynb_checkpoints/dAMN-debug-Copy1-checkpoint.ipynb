{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f6ebc3-26f0-4700-8a61-4ed078ce560f",
   "metadata": {},
   "source": [
    "# dAMN with M28 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b79d9c-a364-4175-910b-0d407998cc80",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f55698-36e4-499f-94cb-0467b3e1d937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.16.2\n",
      "Keras version: 3.9.2\n",
      "/Users/jean-loup-faulon/miniforge3/envs/M4/bin/python\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Keras version:\", tf.keras.__version__)\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b43a9d2c-74b1-441f-824c-d9278c5f52a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_weight: [0.001, 1, 1, 1]\n",
      "loss_decay: [0, 0.5, 0.5, 1]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "config_path = \"./model_july31/M28_OD_20_forecast_0.config.json\"  # July version\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "print(\"loss_weight:\", cfg[\"loss_weight\"])\n",
    "print(\"loss_decay:\", cfg[\"loss_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a9436-caa2-4a83-913b-2f3fbd358fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[process_data] detected OD mode (M28-style OD + DEV)\n",
      "Transport[15,49] = 1.0  EX_met__L_e_o\n",
      "Transport[12,84] = 1.0  EX_ile__L_e_o\n",
      "Transport[13,89] = 1.0  EX_leu__L_e_o\n",
      "Transport[9,96] = 1.0  EX_gln__L_e_o\n",
      "Transport[17,98] = 1.0  EX_pro__L_e_o\n",
      "Transport[23,103] = 1.0  EX_ade_e_o\n",
      "Transport[3,108] = 1.0  EX_ala__L_e_o\n",
      "Transport[4,109] = 1.0  EX_arg__L_e_o\n",
      "Transport[6,110] = 1.0  EX_asp__L_e_o\n",
      "Transport[2,112] = 1.0  EX_succ_e_o\n",
      "Transport[27,113] = 1.0  EX_thymd_e_o\n",
      "Transport[21,115] = 1.0  EX_tyr__L_e_o\n",
      "Transport[8,121] = 1.0  EX_glu__L_e_o\n",
      "Transport[24,125] = 1.0  EX_gua_e_o\n",
      "Transport[0,180] = 1.0  EX_glc__D_e_o\n",
      "Transport[26,186] = 1.0  EX_ura_e_o\n",
      "Transport[22,188] = 1.0  EX_val__L_e_o\n",
      "Transport[1,211] = 1.0  EX_xyl__D_e_o\n",
      "Transport[10,217] = 1.0  EX_gly_e_o\n",
      "Transport[14,219] = 1.0  EX_lys__L_e_o\n",
      "Transport[18,227] = 1.0  EX_ser__L_e_o\n",
      "Transport[20,229] = 1.0  EX_trp__L_e_o\n",
      "Transport[5,1459] = 1.0  EX_asn__L_e_o\n",
      "Transport[16,1489] = 1.0  EX_phe__L_e_o\n",
      "Transport[19,1490] = 1.0  EX_thr__L_e_o\n",
      "Transport[25,1524] = 1.0  EX_csn_e_o\n",
      "Transport[11,1533] = 1.0  EX_his__L_e_o\n",
      "Transport[7,1742] = 1.0  EX_cys__L_e_o\n",
      "Transport[28,2668] = 1.0  BIOMASS\n",
      "Transport[15,2730] = -1.0  EX_met__L_e_i\n",
      "Transport[12,2744] = -1.0  EX_ile__L_e_i\n",
      "Transport[13,2748] = -1.0  EX_leu__L_e_i\n",
      "Transport[9,2753] = -1.0  EX_gln__L_e_i\n",
      "Transport[17,2754] = -1.0  EX_pro__L_e_i\n",
      "Transport[23,2758] = -1.0  EX_ade_e_i\n",
      "Transport[3,2763] = -1.0  EX_ala__L_e_i\n",
      "Transport[4,2764] = -1.0  EX_arg__L_e_i\n",
      "Transport[6,2765] = -1.0  EX_asp__L_e_i\n",
      "Transport[2,2767] = -1.0  EX_succ_e_i\n",
      "Transport[27,2768] = -1.0  EX_thymd_e_i\n",
      "Transport[21,2770] = -1.0  EX_tyr__L_e_i\n",
      "Transport[8,2775] = -1.0  EX_glu__L_e_i\n",
      "Transport[24,2777] = -1.0  EX_gua_e_i\n",
      "Transport[0,2805] = -1.0  EX_glc__D_e_i\n",
      "Transport[26,2808] = -1.0  EX_ura_e_i\n",
      "Transport[22,2809] = -1.0  EX_val__L_e_i\n",
      "Transport[1,2819] = -1.0  EX_xyl__D_e_i\n",
      "Transport[10,2824] = -1.0  EX_gly_e_i\n",
      "Transport[14,2826] = -1.0  EX_lys__L_e_i\n",
      "Transport[18,2829] = -1.0  EX_ser__L_e_i\n",
      "Transport[20,2831] = -1.0  EX_trp__L_e_i\n",
      "Transport[5,3325] = -1.0  EX_asn__L_e_i\n",
      "Transport[16,3342] = -1.0  EX_phe__L_e_i\n",
      "Transport[19,3343] = -1.0  EX_thr__L_e_i\n",
      "Transport[25,3356] = -1.0  EX_csn_e_i\n",
      "Transport[11,3361] = -1.0  EX_his__L_e_i\n",
      "Transport[7,3446] = -1.0  EX_cys__L_e_i\n",
      "Transport shape: (29, 3682), Stoichiometry shape: (1877, 3682)\n",
      "Number of metabolites (k): 29, Number of fluxes (n): 3682\n",
      "Train shape: (280, 377)\n",
      "Val shape: (280, 580)\n",
      "-----------------------------MetabolicModel-----------------------------\n",
      "times: 0.32, 1.57, ..., 23.99\n",
      "metabolite_ids: ['glc__D_e', 'xyl__D_e', 'succ_e', 'ala__L_e', 'arg__L_e', 'asn__L_e', 'asp__L_e', 'cys__L_e', 'glu__L_e', 'gln__L_e', 'gly_e', 'his__L_e', 'ile__L_e', 'leu__L_e', 'lys__L_e', 'met__L_e', 'phe__L_e', 'pro__L_e', 'ser__L_e', 'thr__L_e', 'trp__L_e', 'tyr__L_e', 'val__L_e', 'ade_e', 'gua_e', 'csn_e', 'ura_e', 'thymd_e', 'BIOMASS']\n",
      "Total time step: 20\n",
      "Train time step: 13\n",
      "dt: [1.0000007 1.0000007 1.0000007 1.0000007 1.0000007 1.0000007 1.0000004\n",
      " 1.0000007 1.0000007 1.0000007 1.0000007 1.0000007 1.0000015 1.\n",
      " 1.0000015 1.        1.0000015 1.        1.0000015]\n",
      "Transport: (29, 3682)\n",
      "Stoichiometry: (1877, 3682)\n",
      "n: 3682\n",
      "k: 29\n",
      "Reaction ids: 3682\n",
      "Biomass id: BIOMASS_Ec_iML1515_core_75p37M\n",
      "Biomass flux index: 2668\n",
      "Lag Layer : Hidden size = [50] trainainle parameters = 1602\n",
      "Flux Layer : Hidden size = [500] trainainle parameters = 1859682\n",
      "Dropout Rate: 0.2\n",
      "Loss weight: [0.001, 1, 1, 1]\n",
      "Loss decay: [0, 0.5, 0.5, 1]\n",
      "train_test_split: forecast\n",
      "train_test_split: 13\n",
      "x_fold: 3\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Loading July arrays from:\n",
      "   ./model_july31/M28_OD_20_forecast_val_array.txt\n",
      "   ./model_july31/M28_OD_20_forecast_val_dev.txt\n",
      "   ./model_july31/M28_OD_20_forecast_val_ids.txt\n",
      "July val_array shape: (280, 580)\n",
      "July val_dev shape  : (280, 20)\n",
      "July val_ids shape  : (280,)\n",
      "T_total (from val_array) = 20, k = 29, T_train = 13\n",
      "Reconstructed July train_array shape: (280, 377)\n",
      "Reconstructed July train_dev shape  : (280, 13)\n",
      "\n",
      "=== Sanity check: training data ===\n",
      "C_train_all shape: (280, 13, 29)  (Z, T_train, k)\n",
      "Number of metabolites k = 29\n",
      "Number of time points in training trajectories = 13\n",
      "First 5 metabolite IDs (if available): ['glc__D_e', 'xyl__D_e', 'succ_e']\n",
      "Biomass metabolite ID (last column): BIOMASS\n",
      "C_train_all.shape: (280, 13, 29)\n",
      "\n",
      "Experiment 0 — first time points:\n",
      "t =   0.32  C[0,0,0:3] = [44.404972  0.        0.        0.        0.      ]  BIOMASS = 0.0313 OD = -2.4691\n",
      "t =   1.57  C[0,1,0:3] = [nan nan nan nan nan]  BIOMASS = 0.0313 OD = -2.4691\n",
      "t =   2.81  C[0,2,0:3] = [nan nan nan nan nan]  BIOMASS = 0.0323 OD = -2.4381\n",
      "t =   4.06  C[0,3,0:3] = [nan nan nan nan nan]  BIOMASS = 0.0335 OD = -2.4006\n",
      "t =   5.30  C[0,4,0:3] = [nan nan nan nan nan]  BIOMASS = 0.0360 OD = -2.3296\n",
      "t =   6.55  C[0,5,0:3] = [nan nan nan nan nan]  BIOMASS = 0.0466 OD = -2.0716\n",
      "t =   7.80  C[0,6,0:3] = [nan nan nan nan nan]  BIOMASS = 0.0800 OD = -1.5310\n",
      "t =   9.04  C[0,7,0:3] = [nan nan nan nan nan]  BIOMASS = 0.1627 OD = -0.8218\n",
      "t =  10.29  C[0,8,0:3] = [nan nan nan nan nan]  BIOMASS = 0.2954 OD = -0.2252\n",
      "t =  11.53  C[0,9,0:3] = [nan nan nan nan nan]  BIOMASS = 0.4163 OD = 0.1180\n",
      "t =  12.78  C[0,10,0:3] = [nan nan nan nan nan]  BIOMASS = 0.4244 OD = 0.1371\n",
      "t =  14.02  C[0,11,0:3] = [nan nan nan nan nan]  BIOMASS = 0.4377 OD = 0.1679\n",
      "t =  15.27  C[0,12,0:3] = [nan nan nan nan nan]  BIOMASS = 0.4504 OD = 0.1966\n",
      "\n",
      "=== Sanity check: validation data ===\n",
      "C_val_all shape: (280, 20, 29)  (Z_val, T_val, k)\n",
      "=== End sanity check ===\n",
      "\n",
      "[Epoch 1/1000] Train: s_v=1.6e+00, neg_v=3.0e-01, c=2.3e+00, drop_c=4.4e-02 | Val: s_v=6.5e-01, neg_v=1.1e-01, c=1.9e-01, drop_c=8.9e-03\n",
      "[Epoch 2/1000] Train: s_v=1.6e-01, neg_v=1.1e-01, c=2.2e-01, drop_c=2.8e-02 | Val: s_v=1.2e-01, neg_v=4.1e-02, c=2.5e-02, drop_c=1.0e-03\n",
      "[Epoch 3/1000] Train: s_v=8.3e-02, neg_v=7.0e-02, c=1.3e-01, drop_c=2.1e-02 | Val: s_v=3.0e-02, neg_v=2.6e-02, c=1.1e-02, drop_c=4.8e-04\n",
      "[Epoch 4/1000] Train: s_v=6.1e-02, neg_v=5.4e-02, c=1.1e-01, drop_c=1.7e-02 | Val: s_v=2.6e-02, neg_v=1.9e-02, c=1.1e-02, drop_c=1.5e-03\n",
      "[Epoch 5/1000] Train: s_v=5.4e-02, neg_v=4.4e-02, c=7.9e-02, drop_c=1.5e-02 | Val: s_v=2.1e-02, neg_v=1.7e-02, c=1.5e-02, drop_c=2.8e-04\n",
      "[Epoch 6/1000] Train: s_v=4.4e-02, neg_v=3.8e-02, c=6.9e-02, drop_c=1.4e-02 | Val: s_v=1.7e-02, neg_v=1.5e-02, c=1.3e-02, drop_c=7.1e-05\n",
      "[Epoch 7/1000] Train: s_v=4.1e-02, neg_v=3.4e-02, c=6.5e-02, drop_c=1.2e-02 | Val: s_v=1.7e-02, neg_v=1.5e-02, c=1.4e-02, drop_c=2.6e-05\n",
      "[Epoch 8/1000] Train: s_v=3.6e-02, neg_v=3.1e-02, c=5.3e-02, drop_c=1.0e-02 | Val: s_v=1.6e-02, neg_v=1.4e-02, c=9.5e-03, drop_c=2.7e-04\n",
      "[Epoch 9/1000] Train: s_v=3.2e-02, neg_v=2.9e-02, c=4.3e-02, drop_c=8.6e-03 | Val: s_v=1.5e-02, neg_v=1.3e-02, c=7.1e-03, drop_c=6.2e-05\n",
      "[Epoch 10/1000] Train: s_v=2.9e-02, neg_v=2.7e-02, c=3.8e-02, drop_c=7.9e-03 | Val: s_v=1.3e-02, neg_v=1.2e-02, c=7.0e-03, drop_c=3.4e-05\n",
      "[Epoch 11/1000] Train: s_v=2.6e-02, neg_v=2.5e-02, c=3.6e-02, drop_c=7.4e-03 | Val: s_v=1.1e-02, neg_v=1.2e-02, c=7.0e-03, drop_c=7.9e-05\n"
     ]
    }
   ],
   "source": [
    "# train with july arrays\n",
    "\n",
    "# This cell is slow you should run train.py code\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import data\n",
    "import model\n",
    "import plot\n",
    "print('Physical GPUs:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "train_test_split = 'forecast' # 'forecast' or 'medium'\n",
    "folder = './'\n",
    "file_name = 'M28_OD_20'\n",
    "media_file = folder+'data/'+'M28_media.csv'                 \n",
    "od_file = folder+'data/'+file_name+'.csv'               \n",
    "cobra_model_file = folder+'data/'+'iML1515_duplicated.xml'\n",
    "biomass_rxn_id = 'BIOMASS_Ec_iML1515_core_75p37M'\n",
    "\n",
    "# Hyperparameters (same as July)\n",
    "seed = 10\n",
    "np.random.seed(seed=seed)\n",
    "hidden_layers_lag = [50]\n",
    "hidden_layers_flux = [500]\n",
    "num_epochs = 1000\n",
    "x_fold = 3    \n",
    "batch_size = 10\n",
    "patience = 100\n",
    "N_iter = 3\n",
    "run_name = f'{file_name}_{train_test_split}'\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Build the model using the CURRENT code, but ignore the arrays it returns\n",
    "#    (we'll overwrite them with the July arrays below).\n",
    "# -------------------------------------------------------------------\n",
    "mdl, _, _, _, _, _ = model.create_model_train_val(\n",
    "    media_file, od_file,\n",
    "    cobra_model_file,\n",
    "    biomass_rxn_id,\n",
    "    x_fold=x_fold, \n",
    "    hidden_layers_lag=hidden_layers_lag, \n",
    "    hidden_layers_flux=hidden_layers_flux, \n",
    "    dropout_rate=0.2,\n",
    "    loss_weight=[0.001, 1, 1, 1], # july\n",
    "    loss_decay=[0, 0.5, 0.5, 1],\n",
    "    verbose=True,\n",
    "    train_test_split=train_test_split\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Load the EXACT July arrays (old prefix: \"M28o_...\") and reconstruct\n",
    "#    July's train_array and train_dev from them.\n",
    "# -------------------------------------------------------------------\n",
    "old_prefix = f'{folder}model_july31/{file_name}_{train_test_split}'\n",
    "# If your July files are named differently, adjust this pattern accordingly.\n",
    "\n",
    "old_val_array_path = old_prefix + '_val_array.txt'\n",
    "old_val_dev_path   = old_prefix + '_val_dev.txt'\n",
    "old_val_ids_path   = old_prefix + '_val_ids.txt'\n",
    "\n",
    "print(\"\\nLoading July arrays from:\")\n",
    "print(\"  \", old_val_array_path)\n",
    "print(\"  \", old_val_dev_path)\n",
    "print(\"  \", old_val_ids_path)\n",
    "\n",
    "val_array = np.loadtxt(old_val_array_path, dtype=float)   # shape (Z, T_total * k)\n",
    "val_dev   = np.loadtxt(old_val_dev_path,   dtype=float)   # shape (Z, T_total)\n",
    "val_ids   = np.loadtxt(old_val_ids_path,  dtype=int)      # shape (Z,)\n",
    "\n",
    "print(\"July val_array shape:\", val_array.shape)\n",
    "print(\"July val_dev shape  :\", val_dev.shape)\n",
    "print(\"July val_ids shape  :\", val_ids.shape)\n",
    "\n",
    "# Infer T_total and reconstruct July train_array / train_dev\n",
    "Z, total_flat = val_array.shape\n",
    "k = mdl.k\n",
    "T_total = total_flat // k\n",
    "T_train = mdl.train_time_steps   # this was computed inside create_model_train_val\n",
    "\n",
    "print(f\"T_total (from val_array) = {T_total}, k = {k}, T_train = {T_train}\")\n",
    "\n",
    "# Reshape val_array to (Z, T_total, k)\n",
    "val_array_3d = val_array.reshape(Z, T_total, k)\n",
    "\n",
    "# July forecast split: train_array = first T_train time steps (for all experiments)\n",
    "train_array = val_array_3d[:, :T_train, :].reshape(Z, T_train * k)\n",
    "\n",
    "# July train_dev: first T_train time points, val_dev: all time points\n",
    "train_dev = val_dev[:, :T_train]\n",
    "\n",
    "print(\"Reconstructed July train_array shape:\", train_array.shape)\n",
    "print(\"Reconstructed July train_dev shape  :\", train_dev.shape)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) (Optional) Save the arrays again under the \"current\" prefix, if you want\n",
    "#    to keep them for later sanity checks:\n",
    "# -------------------------------------------------------------------\n",
    "np.savetxt(f'{folder}model/{run_name}_val_array.txt', val_array, fmt='%f')\n",
    "np.savetxt(f'{folder}model/{run_name}_val_dev.txt',   val_dev,   fmt='%f')\n",
    "np.savetxt(f'{folder}model/{run_name}_val_ids.txt',   np.asarray(val_ids), fmt='%d')\n",
    "np.savetxt(f'{folder}model/{run_name}_train_array.txt', train_array, fmt='%f')\n",
    "np.savetxt(f'{folder}model/{run_name}_train_dev.txt',   train_dev,   fmt='%f')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Train model on the *July* arrays\n",
    "# -------------------------------------------------------------------\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=280,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True\n",
    "    )\n",
    "(\n",
    "    (losses_s_v_train, losses_neg_v_train, losses_c_train, losses_drop_c_train),\n",
    "    (losses_s_v_val,   losses_neg_v_val,   losses_c_val,   losses_drop_c_val)\n",
    "    ) = model.train_model(\n",
    "        mdl, train_array, val_array=val_array,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        num_epochs=num_epochs, batch_size=batch_size, patience=patience,\n",
    "        verbose=True,\n",
    "        train_test_split=train_test_split,\n",
    "        x_fold=x_fold\n",
    "    )\n",
    "\n",
    "for i in range(N_iter):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=280,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True\n",
    "    )\n",
    "    (\n",
    "        (losses_s_v_train, losses_neg_v_train, losses_c_train, losses_drop_c_train),\n",
    "        (losses_s_v_val,   losses_neg_v_val,   losses_c_val,   losses_drop_c_val)\n",
    "    ) = model.train_model(\n",
    "        mdl, train_array, val_array=val_array,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        num_epochs=num_epochs, batch_size=batch_size, patience=patience,\n",
    "        verbose=True,\n",
    "        train_test_split=train_test_split,\n",
    "        x_fold=x_fold\n",
    "    )\n",
    "\n",
    "    mdl_name = f'{folder}model/{run_name}_{str(i)}'\n",
    "    mdl.save_model(model_name=mdl_name, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5123d2-e662-496d-af9d-bcd9c4e5e1cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old config exists : True\n",
      "Old weights exist: True\n",
      "New config exists : True\n",
      "New weights exist: True\n",
      "\n",
      "=== CONFIG DIFF ===\n",
      "UB_in:\n",
      "  old = <MISSING>\n",
      "  new = 0.0\n",
      "UB_out:\n",
      "  old = <MISSING>\n",
      "  new = 0.0\n",
      "metabolite_ids:\n",
      "  old = <MISSING>\n",
      "  new = ['glc__D_e', 'xyl__D_e', 'succ_e', 'ala__L_e', 'arg__L_e', 'asn__L_e', 'asp__L_e', 'cys__L_e', 'glu__L_e', 'gln__L_e', 'gly_e', 'his__L_e', 'ile__L_e', 'leu__L_e', 'lys__L_e', 'met__L_e', 'phe__L_e', 'pro__L_e', 'ser__L_e', 'thr__L_e', 'trp__L_e', 'tyr__L_e', 'val__L_e', 'ade_e', 'gua_e', 'csn_e', 'ura_e', 'thymd_e', 'BIOMASS']\n",
      "times:\n",
      "  old = [0.322222222, 1.5678362571578943, 2.81345029231579, 4.059064327473684, 5.304678362631578, 6.550292397789472, 7.795906432947367, 9.041520468105263, 10.287134503263156, 11.532748538421052, 12.778362573578946, 14.023976608736842, 15.269590643894736, 16.51520467905263, 17.760818714210526, 19.00643274936842, 20.25204678452631, 21.497660819684207, 22.743274854842102, 23.98888889]\n",
      "  new = [0.3222222328186035, 1.5678362846374512, 2.813450336456299, 4.0590643882751465, 5.304678440093994, 6.550292491912842, 7.7959065437316895, 9.041520118713379, 10.287134170532227, 11.532748222351074, 12.778362274169922, 14.02397632598877, 15.269590377807617, 16.51520538330078, 17.760818481445312, 19.006433486938477, 20.252046585083008, 21.497661590576172, 22.743274688720703, 23.988889694213867]\n",
      "=== END CONFIG DIFF ===\n",
      "\n",
      "Loading old model...\n",
      "\n",
      "Old model params: 1861284\n",
      "New model params: 1861284\n",
      "\n",
      "=== WEIGHT COMPARISON ===\n",
      "Number of weight arrays: old = 8  new = 8\n",
      "Layer 0: ||old-new|| = 1.404e+01, ||old|| = 8.575e+00, ||new|| = 1.134e+01\n",
      "Layer 1: ||old-new|| = 1.811e+00, ||old|| = 4.334e-01, ||new|| = 1.638e+00\n",
      "Layer 2: ||old-new|| = 3.655e+00, ||old|| = 1.971e+00, ||new|| = 2.953e+00\n",
      "Layer 3: ||old-new|| = 7.202e-01, ||old|| = 1.824e-02, ||new|| = 7.144e-01\n",
      "Layer 4: ||old-new|| = 1.633e+01, ||old|| = 9.633e+00, ||new|| = 1.339e+01\n",
      "Layer 5: ||old-new|| = 1.022e+00, ||old|| = 1.473e+00, ||new|| = 1.621e+00\n",
      "Layer 6: ||old-new|| = 4.124e+01, ||old|| = 3.528e+01, ||new|| = 4.018e+01\n",
      "Layer 7: ||old-new|| = 1.095e+00, ||old|| = 4.334e+00, ||new|| = 4.228e+00\n",
      "=== END WEIGHT COMPARISON ===\n",
      "\n",
      "val_array shape: (280, 580)\n",
      "\n",
      "Check that references match (they should): True\n",
      "\n",
      "Old model: mean R2 = 0.935\n",
      "New model: mean R2 = 0.929\n",
      "\n",
      "Prediction difference stats (new - old):\n",
      "  abs mean: 0.866044\n",
      "  abs max : 10.509622\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import model\n",
    "import utils  # if you need r2_growth_curve later\n",
    "import model  # if you need r2_growth_curve later\n",
    "import os\n",
    "\n",
    "metabolite_ids = [\n",
    "'glc__D_e', 'xyl__D_e', 'succ_e', 'ala__L_e', 'arg__L_e', 'asn__L_e', 'asp__L_e',\n",
    "'cys__L_e', 'glu__L_e', 'gln__L_e', 'gly_e', 'his__L_e', 'ile__L_e', 'leu__L_e',\n",
    "'lys__L_e', 'met__L_e', 'phe__L_e', 'pro__L_e', 'ser__L_e', 'thr__L_e', 'trp__L_e',\n",
    "'tyr__L_e', 'val__L_e', 'ade_e', 'gua_e', 'csn_e', 'ura_e', 'thymd_e', 'BIOMASS'\n",
    "]\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Paths to the two models you want to compare\n",
    "# -------------------------------------------------------------------\n",
    "# Example: old = July model, new = current retrain\n",
    "old_base = \"./model_july31/M28_OD_20_forecast_1\"   # <--- adjust to your old files\n",
    "new_base = \"./model/M28_OD_20_forecast_1\"        # <--- adjust to your new files\n",
    "\n",
    "old_config_path = old_base + \".config.json\"\n",
    "new_config_path = new_base + \".config.json\"\n",
    "\n",
    "old_weights_path = old_base + \".weights.h5\"\n",
    "new_weights_path = new_base + \".weights.h5\"\n",
    "\n",
    "print(\"Old config exists :\", os.path.exists(old_config_path))\n",
    "print(\"Old weights exist:\", os.path.exists(old_weights_path))\n",
    "print(\"New config exists :\", os.path.exists(new_config_path))\n",
    "print(\"New weights exist:\", os.path.exists(new_weights_path))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Load and diff configs\n",
    "# -------------------------------------------------------------------\n",
    "def load_config(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "old_cfg = load_config(old_config_path)\n",
    "new_cfg = load_config(new_config_path)\n",
    "\n",
    "print(\"\\n=== CONFIG DIFF ===\")\n",
    "all_keys = sorted(set(old_cfg.keys()) | set(new_cfg.keys()))\n",
    "for k in all_keys:\n",
    "    v_old = old_cfg.get(k, \"<MISSING>\")\n",
    "    v_new = new_cfg.get(k, \"<MISSING>\")\n",
    "    if v_old != v_new:\n",
    "        print(f\"{k}:\")\n",
    "        print(f\"  old = {v_old}\")\n",
    "        print(f\"  new = {v_new}\")\n",
    "print(\"=== END CONFIG DIFF ===\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Load models from disk using your MetabolicModel.load_model\n",
    "# -------------------------------------------------------------------\n",
    "print(\"Loading old model...\")\n",
    "new_model = model.MetabolicModel.load_model(model_name=new_base, metabolite_ids=metabolite_ids, verbose=False)\n",
    "old_model = model.MetabolicModel.load_model(model_name=old_base, metabolite_ids=metabolite_ids, verbose=False)\n",
    "\n",
    "print(\"\\nOld model params:\", old_model.count_params())\n",
    "print(\"New model params:\", new_model.count_params())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Compare weights (only if architectures match)\n",
    "# -------------------------------------------------------------------\n",
    "old_weights = old_model.get_weights()\n",
    "new_weights = new_model.get_weights()\n",
    "\n",
    "print(\"\\n=== WEIGHT COMPARISON ===\")\n",
    "print(\"Number of weight arrays: old =\", len(old_weights), \" new =\", len(new_weights))\n",
    "if len(old_weights) != len(new_weights):\n",
    "    print(\"WARNING: different number of weight tensors → architectures changed.\")\n",
    "else:\n",
    "    for i, (ow, nw) in enumerate(zip(old_weights, new_weights)):\n",
    "        if ow.shape != nw.shape:\n",
    "            print(f\"Layer {i}: shape mismatch old {ow.shape} vs new {nw.shape}\")\n",
    "            continue\n",
    "        diff = np.linalg.norm(ow - nw)\n",
    "        n_old = np.linalg.norm(ow)\n",
    "        n_new = np.linalg.norm(nw)\n",
    "        print(f\"Layer {i}: ||old-new|| = {diff:.3e}, ||old|| = {n_old:.3e}, ||new|| = {n_new:.3e}\")\n",
    "print(\"=== END WEIGHT COMPARISON ===\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) Compare predictions on the SAME validation data\n",
    "# -------------------------------------------------------------------\n",
    "val_array_path = \"./model_july31/M28_OD_20_forecast_val_array.txt\"  # adjust if name differs\n",
    "\n",
    "val_array = np.loadtxt(val_array_path, dtype=float)\n",
    "print(\"val_array shape:\", val_array.shape)\n",
    "\n",
    "# Old model predictions\n",
    "old_pred, old_ref = model.predict_on_val_data(old_model, val_array, verbose=False)\n",
    "old_pred = np.asarray(old_pred)\n",
    "old_ref  = np.asarray(old_ref)\n",
    "\n",
    "# New model predictions\n",
    "new_pred, new_ref = model.predict_on_val_data(new_model, val_array, verbose=False)\n",
    "new_pred = np.asarray(new_pred)\n",
    "new_ref  = np.asarray(new_ref)\n",
    "\n",
    "print(\"\\nCheck that references match (they should):\",\n",
    "      np.allclose(old_ref, new_ref, equal_nan=True))\n",
    "\n",
    "# Example: growth-curve R² (like you already do)\n",
    "OD = True\n",
    "R2_old = utils.r2_growth_curve(np.expand_dims(old_pred, 0),\n",
    "                               np.expand_dims(old_ref, 0),\n",
    "                               OD=OD)\n",
    "R2_new = utils.r2_growth_curve(np.expand_dims(new_pred, 0),\n",
    "                               np.expand_dims(new_ref, 0),\n",
    "                               OD=OD)\n",
    "\n",
    "print(f\"\\nOld model: mean R2 = {np.mean(R2_old):.3f}\")\n",
    "print(f\"New model: mean R2 = {np.mean(R2_new):.3f}\")\n",
    "\n",
    "# Also inspect numeric difference in predictions\n",
    "diff_pred = new_pred - old_pred\n",
    "print(\"\\nPrediction difference stats (new - old):\")\n",
    "print(\"  abs mean:\", np.nanmean(np.abs(diff_pred)))\n",
    "print(\"  abs max :\", np.nanmax(np.abs(diff_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a21ce-31bc-4511-a6fe-020f061f33f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (M4)",
   "language": "python",
   "name": "m4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
